name: ML Pipeline

on:
  push:
    branches: [ main ]
    paths:
      - 'ml/**'
      - 'internal/scoring/**'
      - 'internal/summarizer/**'
  pull_request:
    branches: [ main ]
    paths:
      - 'ml/**'
      - 'internal/scoring/**'
      - 'internal/summarizer/**'
  schedule:
    # Run weekly on Sundays at 3 AM UTC
    - cron: '0 3 * * 0'

env:
  MLFLOW_TRACKING_URI: ${{ secrets.MLFLOW_TRACKING_URI }}
  AWS_ACCESS_KEY_ID: ${{ secrets.AWS_ACCESS_KEY_ID }}
  AWS_SECRET_ACCESS_KEY: ${{ secrets.AWS_SECRET_ACCESS_KEY }}

jobs:
  # ============================================
  # Job 1: Data Quality Checks
  # ============================================
  data-quality:
    name: Data Quality Checks
    runs-on: ubuntu-latest
    steps:
    - name: Checkout code
      uses: actions/checkout@v4

    - name: Set up Python
      uses: actions/setup-python@v5
      with:
        python-version: '3.11'

    - name: Install dependencies
      run: |
        pip install great_expectations pandas sqlalchemy psycopg2-binary

    - name: Run Great Expectations
      run: |
        if [ ! -d ml/data_quality ]; then
          echo "ml/data_quality not found, skipping data quality checkpoint"
          exit 0
        fi
        cd ml/data_quality
        great_expectations checkpoint run content_items_checkpoint

    - name: Upload Data Docs
      if: hashFiles('ml/data_quality/**') != ''
      uses: actions/upload-artifact@v4
      with:
        name: data-docs
        path: ml/data_quality/gx/uncommitted/data_docs/

  # ============================================
  # Job 2: Model Training
  # ============================================
  train-model:
    name: Train Ranking Model
    runs-on: ubuntu-latest
    needs: data-quality
    steps:
    - name: Checkout code
      uses: actions/checkout@v4

    - name: Set up Python
      uses: actions/setup-python@v5
      with:
        python-version: '3.11'

    - name: Install dependencies
      run: |
        pip install mlflow pandas scikit-learn numpy

    - name: Train model
      run: |
        if [ ! -d ml/training ]; then
          echo "ml/training not found, skipping training step"
          exit 0
        fi
        cd ml/training
        python train_ranking_model.py

    - name: Log metrics to MLflow
      run: |
        if [ ! -d ml/training ]; then
          echo "ml/training not found, skipping MLflow logging"
          exit 0
        fi
        cd ml/training
        python log_experiment.py

    - name: Upload model artifacts
      if: hashFiles('ml/models/**') != ''
      uses: actions/upload-artifact@v4
      with:
        name: model-artifacts
        path: ml/models/

  # ============================================
  # Job 3: Model Evaluation
  # ============================================
  evaluate-model:
    name: Evaluate Model
    runs-on: ubuntu-latest
    needs: train-model
    steps:
    - name: Checkout code
      uses: actions/checkout@v4

    - name: Set up Python
      uses: actions/setup-python@v5
      with:
        python-version: '3.11'

    - name: Install dependencies
      run: |
        pip install mlflow pandas scikit-learn

    - name: Evaluate model
      run: |
        cd ml/evaluation
        python evaluate_model.py

    - name: Compare with production model
      run: |
        cd ml/evaluation
        python compare_models.py

    - name: Upload evaluation report
      uses: actions/upload-artifact@v4
      with:
        name: evaluation-report
        path: ml/evaluation/reports/

  # ============================================
  # Job 4: Model Validation
  # ============================================
  validate-model:
    name: Model Validation
    runs-on: ubuntu-latest
    needs: evaluate-model
    steps:
    - name: Checkout code
      uses: actions/checkout@v4

    - name: Set up Python
      uses: actions/setup-python@v5
      with:
        python-version: '3.11'

    - name: Run model validation tests
      run: |
        cd ml/validation
        python validate_model.py

    - name: Check fairness metrics
      run: |
        cd ml/validation
        python fairness_check.py

  # ============================================
  # Job 5: Deploy Model (Staging)
  # ============================================
  deploy-model-staging:
    name: Deploy to Staging
    runs-on: ubuntu-latest
    needs: [train-model, evaluate-model, validate-model]
    environment: staging
    steps:
    - name: Checkout code
      uses: actions/checkout@v4

    - name: Configure AWS credentials
      uses: aws-actions/configure-aws-credentials@v4
      with:
        aws-access-key-id: ${{ secrets.AWS_ACCESS_KEY_ID }}
        aws-secret-access-key: ${{ secrets.AWS_SECRET_ACCESS_KEY }}
        aws-region: us-west-2

    - name: Download model artifacts
      uses: actions/download-artifact@v7
      with:
        name: model-artifacts
        path: ml/models/

    - name: Upload model to S3
      run: |
        aws s3 cp ml/models/ s3://evolipia-radar-ml-models/staging/ --recursive

    - name: Update model version in staging
      run: |
        aws ssm put-parameter           --name /evolipia-radar/staging/model-version           --value ${{ github.sha }}           --type String           --overwrite

    - name: Trigger staging deployment
      run: |
        aws eks update-kubeconfig --name evolipia-radar-staging
        kubectl rollout restart deployment/worker -n staging

  # ============================================
  # Job 6: A/B Test Analysis
  # ============================================
  ab-test-analysis:
    name: A/B Test Analysis
    runs-on: ubuntu-latest
    if: github.event_name == 'schedule'
    steps:
    - name: Checkout code
      uses: actions/checkout@v4

    - name: Set up Python
      uses: actions/setup-python@v5
      with:
        python-version: '3.11'

    - name: Install dependencies
      run: |
        pip install pandas scipy numpy matplotlib

    - name: Run A/B test analysis
      run: |
        cd ml/experiments
        python ab_test_analysis.py

    - name: Upload results
      uses: actions/upload-artifact@v4
      with:
        name: ab-test-results
        path: ml/experiments/results/
